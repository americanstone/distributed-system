<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">

<title>
6.824 Spring 2012 Paper Questions
</title>

<!-- To add a new question, just put in within a <div> tag, give it
some identifier (i.e., 'ID="qXX"'), and then add it to the questions
array variable below.  To link directly to the question, just use a
link to 'questions.html?q=qXX'.
 -->

<script Language="Javascript">
<!--
var questions = new Array( "qgo",
                           "qhypervisor",
			   "qfds",
			   "qpaxos",
			   "qharp",
			   "qspanner",
			   "qsundr",
			   "qivy",
			   "qtreadmarks",
			   "qficus",
			   "qbayou",
			   "qpnuts",
			   "qdynamo",
			   "qargus88",
                           "qplan9",
                           "qmr",
                           "qbitcoin",
                           "qmemcached",
                           "qanalogic");

function init_questions() {
    var parts = new Array();
    parts = location.href.split('?');
    if( parts.length > 1 ) {
        var qparts = new Array();
	qparts = parts[1].split("=");
	for( var i = 0; i < questions.length; i++ ) {
	  if( qparts[1] != questions[i] ) {
	    document.getElementById(questions[i]).innerHTML = '';
	  }
	}
	document.getElementById("viewall").innerHTML = "<p><a href='questions.html'>View all questions</a>";
    }
}
-->
</script>

</head>

<body bgcolor="#ffffff" text="#00000" onLoad="javascript:init_questions();">

<div align="center">
<h2>
<a href="index.html">6.824</a> Spring 2012 Paper Questions
</h2>
</div>

<a name="top"></a>
<p>
Please submit your answer for each day's paper question by the
beginning of
class: <a href="https://narula.scripts.mit.edu:444/6.824/handin.py">https://narula.scripts.mit.edu:444/6.824/handin.py</a>.

<p>You can also upload a file using curl:
<pre>
$ curl -F file=@lec2.txt \
        -F key=XXXXXXXX \
        http://narula.scripts.mit.edu/6.824/handin.py/upload</pre>
<p>

<div ID="qgo">
<p>

<a href="http://tour.golang.org/">Go tour</a>:
what does this Go program print?

<pre>
package main
import "fmt"
func f(x int) func() int {
  return func() int { x++; return x }
}
func main() {
  z := f(10)
  z()
  fmt.Printf("%v\n", z())
}
</pre>
</div>

<div ID="qhypervisor">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/bressoud-hypervisor.pdf">Hypervisor-based
  Fault-tolerance</a>
Suppose that instead of connecting both the primary and backup to the
same disk, we connected them to separate disks with identical copies
of the data? Would this work? What else might we have to worry about,
and what are some things that could go wrong?
</div>

<div ID="qfds">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/fds.pdf">Flat Datacenter Storage</a> Suppose
tractserver T1 is temporarily unreachable due to a network problem, so
the metadata server drops T1 from the TLT.
Then the network problem goes away, but for a while the metadata
server is not aware that T1's status has changed. During this time
could T1 serve client
requests to read and write tracts that it stores? If yes, give an
example of how this could happen. If no, explain what mechanism(s)
prevent this from happening.
</div>

<div ID="qpaxos">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/paxos-simple.pdf">Paxos Made Simple</a>
Suppose that the acceptors are <i>A</i>, <i>B</i>,
and <i>C</i>. <i>A</i> and <i>B</i> are also
proposers.
How does Paxos
ensure that the following sequence of events <u>can't</u> happen?
What actually happens, and which value is ultimately chosen?

<ol>
<li> <i>A</i> sends prepare requests with proposal number 1, and gets responses
  from <i>A</i>, <i>B</i>, and <i>C</i>.
<li> <i>A</i> sends <tt><nobr>accept(1, "foo")</nobr></tt>
  to <i>A</i> and <i>C</i> and gets responses from both.
  Because a majority accepted, <i>A</i> thinks
  that <tt>"foo"</tt> has been chosen.
  However, <i>A</i> crashes before sending an <tt>accept</tt> to <i>B</i>.
<li><i>B</i> sends prepare messages with proposal number 2, and gets responses
  from <i>B</i> and <i>C</i>.
<li><i>B</i> sends <tt><nobr>accept(2, "bar")</nobr></tt> messages
  to <i>B</i> and <i>C</i> and gets responses
  from both, so <i>B</i> thinks that <tt>"bar"</tt> has been chosen.
</ol>
</div>

<div ID="qharp">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/bliskov-harp.pdf">Replication in the Harp File System</a>
Figures 5-1, 5-2, and 5-3 show that Harp often finishes benchmarks
faster than a conventional non-replicated NFS server. This may be
surprising, since you might expect Harp to do strictly more work than a
conventional NFS server (for example, Harp must manage the replication).
Why is Harp often faster? Will all NFS operations be faster with Harp
than on a conventional NFS server, or just some of them? Which?
</div>

<div ID="qspanner">
<p><a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/spanner.pdf">Spanner</a>
Suppose a Spanner server's TT.now() returns correct
information, but the uncertainty is large. For example,
suppose the absolute time is 10:15:30, and TT.now()
returns the interval [10:15:20,10:15:40]. That interval
is correct in that it contains the absolute time, but the
error bound is 10 seconds.
See Section 3 for an explanation TT.now().
What bad effect will a large error bound have on Spanner's operation?
Give a specific example.
</div>

<div ID="qsundr">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/li-sundr.pdf">Secure Untrusted Data Repository (SUNDR)</a>
In the simple straw-man, both fetch and modify operations are placed
in the log and signed. Suppose an alternate design that only signs and
logs modify operations. Does this allow a malicious server to break
fetch-modify consistency or fork consistency? Why or why not?
</div>

<div ID="qivy">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/li-dsm.pdf">Memory Coherence in Shared Virtual Systems</a>
<a href="notes/ivy-code.txt">ivy-code.txt</a> is a version of
the code in Section 3.1 with some clarifications and bug fixes.
The manager part of the WriteServer sends out invalidate messages,
and waits for confirmation messages indicating that the invalidates
have been received and processed. Suppose the manager send out
invalidates, but did not wait for confirmations.
Describe a scenario in which
lack of the confirmation would cause the system to behave
incorrectly. You should assume that the network delivers all messages,
and that none of the computers fail.
</div>

<div ID="qtreadmarks">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/keleher-treadmarks.pdf">Distributed Shared Memory on
  Standard Workstations and Operating Systems</a>
Suppose that a simplified version of Treadmarks, called Dreadmarks,
simply sent all modifications of variables between an acquire and a
release to the next processor to acquire the same lock. No other
modifications are sent. What changes does Treadmarks send that
Dreadmarks does not?  Outline a specific simple situation in which
Treadmarks would provide more useful or intuitive memory behavior than
Dreadmarks.
</div>

<div ID="qficus">
<p><a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/ficus.pdf">Ficus</a>

Imagine a situation like the paper's Figure 1, but in which only Site
A updates file Foo. What should Ficus do in that case when the
partition is merged? Explain how Ficus could tell the difference
between the situation in which both Site A and Site B update Foo,
and the situation in which only Site A updates Foo.

</div>

<div ID="qbayou">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/bayou-conflicts.ps">Managing Update Conflicts in Bayou</a>
  Suppose we build a distributed filesystem using Bayou, and the
  system has a copy operation. Initially, file A contains "foo"
  and file B contains "bar". On one node, a user copies file A
  to file B, overwriting the old contents of B. On another node, a
  user copies file B to file A. After both operations are committed, we
  want both files to contain "foo" or for both files to contain
  "bar". Sketch a dependency check and merge procedure for the
  copy operation that makes this work. How does Bayou ensure that
  all the nodes agree about whether A and B contain "foo" or
  "bar"?
</div>

<div ID="qpnuts">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/cooper-pnuts.pdf">PNUTS: Yahoo!'s Hosted Data Serving Platform</a>
Briefly explain why it is (or isn't) okay to use relaxed consistency
for social applications (see Section 4). Does PNUTS handle the type of
problem presented by Example 1 in Section 1, and if so, how?
</div>

<div ID="qdynamo">
<p><a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/dynamo.pdf">Dynamo</a>
Suppose Dynamo server S1 is perfectly healthy with a working network
connection. By mistake, an administrator instructs server S2 to remove
S1 using the mechanisms described in 4.8.1 and 4.9. It takes a while
for the membership change to propagate from S2 to the rest of the
system (including S1), so for a while some clients and servers will
think that S1 is still part of the system. Will Dynamo operate
correctly in this situation? Why, or why not?
</div>

<!-- I presume this is an old question that we don't want... -->
<!--
<div ID="xqargus">
<p>
<a href="papers/liskov-argus.pdf">Guardians and Actions: Linguistic Support for Robust, Distributed Programs</a>
In Figure 4, <tt>read_mail</tt> deletes email from the mailbox.  If
new mail arrives just before the delete, will the email be deleted but
not returned?  Please briefly explain your answer.
</div>
-->

<div ID="qargus88">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/argus88.pdf">Distributed Programming in Argus</a>
Starting at the bottom-left of page 310, the paper mentions that
a participant writes new versions to disk <b>twice</b>:
once before replying to a prepare message, and once after receiving
a commit message.
Why are both writes necessary?
What could go wrong if participants replied to the prepare
without writing the disk, instead only writing the disk
after receiving a commit message?
</div>

<div ID="qplan9">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/plan9.pdf">Plan 9 from Bell Labs</a>
List three features introduced by Plan 9 that have not been adopted
by today's common operating systems.  For each, why do you think
the idea hasn't become popular?
</div>

<div ID="qmr">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/mapreduce.pdf">MapReduce</a>
How soon after it receives the first file of intermediate data can a
reduce worker start calling the application's Reduce function?
Explain your answer.
</div>

<div ID="qbitcoin">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/bitcoin.pdf">Bitcoin</a>
Try to buy something with Bitcoin. It may help to cooperate
with some 6.824 class-mates, and it may help to start a
few days early.
If you decide to give up, that's OK. Briefly describe your experience.
</div>

<div ID="qmemcached">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/memcache-fb.pdf">Memcache at Facebook</a>.
Section 3.3 implies that a client that writes data does not
delete the corresponding key from the Gutter servers, even
though the client does try to delete the key from the ordinary
Memcached servers (Figure 1). Explain why it would be a bad
idea for writing clients to delete keys from Gutter servers.
</div>

<div ID="qanalogic">
<p>
<a href="https://pdos.csail.mit.edu/archive/6.824-2013/papers/katabi-analogicfs.pdf">Experiences with a Distributed,
 Scalable, Methodological File System: AnalogicFS</a>. In many ways,
 this experiences paper raises more questions than it answers. Please
 answer one of the following questions, taking into consideration the
 rich history of AnalogicFS and the spirit in which the paper was
 written:

<p>
a) The analysis of A* search shown in Figure 1 claims to be an
introspective visualization of the AnalogicFS methodology; however,
not all decisions are depicted in the figure. In particular, if I
<= P, what should be the next node explored such that all assumptions
in Section 2 still hold? Show your work.

<p>
b) Despite the authors' claims in the introduction that AnalogicFS was
developed to study SCSI disks (and their interaction with lambda
calculus), the experimental setup detailed in Section 4.1 involves
decommissioned Gameboys instead, which use cartridge-based, Flash-like
memory. If the authors had used actual SCSI disks during the
experiments, how exactly might have their results changed
quantitatively?

<p>
c) AnalogicFS shows rather unstable multicast algorithm popularity
(Figure 5), especially compared with some of the previous systems
we've read about in 6.824. Give an example of another system that
would have a more steady measurement of popularity pages, especially
in the range of 0.1-0.4 decibels of bandwidth.

<p>
d) For his 6.824 project, Ben Bitdiddle chose to build a 
variant of Lab 4
that faithfully emulates the constant expected seek time across
LISP machines, as AnalogicFS does. Upon implementation, however, he
immediately ran into the need to cap the value size to 400 nm, rather
than 676 nm. Explain what assumptions made for the AnalogicFS
implementation do not hold true for Lab 4, and why that changes the
maximum value size.

</div>


<div ID="viewall"></div>

<hr>
<p>Questions or comments regarding 6.824? Send e-mail to <a href=
"mailto:6.824-staff@pdos.csail.mit.edu"><i>6.824-staff@pdos.csail.mit.edu</i></a>.</p>

<p><b><a href="#top">Top</a></b> //
<b><a href= "index.html">6.824 home</a></b> //</p>
</body>
</html>
